# [Exploring the Different Types of Clustering Algorithms in Machine Learning with Python](https://www.thepythoncode.com/article/clustering-algorithms-in-machine-learning-with-python)
##
# [[] / []]()
Знакомство
Данные являются сердцем алгоритмов машинного обучения. Он определяет, как будет работать алгоритм, что он будет выводить и как он будет полезен в реальном мире. Говоря с точки зрения маркировки, вы можете иметь три типа данных, как указано ниже.

Помеченные данные: как следует из названия, все образцы данных будут помечены определенным классом. Трудно найти помеченные данные в реальной жизни, и часто дорого.
Немаркированные данные: в этой категории ни один из образцов данных не помечен.
Полуметифицированные данные: когда некоторые точки данных помечены, а некоторые не помечены, они называются полумеблированными данными.
В реальном мире вы очень часто найдете немаркированные данные. Это связано с тем, что маркировка данных требует опыта и времени, которые являются дорогостоящими.

Поэтому специалисты разработали методы работы с немаркированными и полумаркированными данными. Кластеризация является одним из таких методов. Он направлен на поиск скрытых шаблонов в данных без необходимости каких-либо меток.

В этой статье рассматриваются алгоритмы кластеризации в машинном обучении, включая классические алгоритмы кластеризации и недавно разработанные методы, примеры кодов каждого алгоритма и их результаты на образцах наборов данных. Но давайте сначала разберемся, что такое кластеризация и как она работает.

Содержание
Кластеризация и ее потребность в машинном обучении
Типы кластеризации
Кластеризация на основе плотности
Кластеризация на основе распределения
Кластеризация на основе центроидов
Иерархическая кластеризация
Другие классификации алгоритмов кластеризации
Жесткая кластеризация и мягкая кластеризация
Монотетическая кластеризация и полиэтиленовая кластеризация
Популярные алгоритмы кластеризации
Установка библиотеки
Кластеризация K-средних
Мини-пакетная кластеризация K-означает
Распространение аффинити
Алгоритм кластеризации DBSCAN
ОПТИКА
БЕРЕЗА
Агломеративная кластеризация
Среднесмещатая кластеризация
Спектральная кластеризация
Модель смеси Гаусса
Метрики для оценки алгоритмов кластеризации
Оценка однородности
Оценка полноты
V Оценка измерения
Примеры использования кластеризации
Лучший алгоритм кластеризации и вещи, которые следует иметь в виду
Заключение
Кластеризация и ее потребность в машинном обучении
Кластеризация

Рис 1: Кластеризация

Как следует из названия, кластеризация буквально означает группировку похожих вещей вместе и разделение разных вещей. Кластеризация в группах машинного обучения похожих точек данных, разделяющих различные точки данных.

Как упоминалось выше, много раз вы столкнетесь с немаркированными данными в реальном мире, и получение данных от экспертов занимает много времени и дорого. В таких сценариях невозможно применить контролируемые алгоритмы машинного обучения.

Тем не менее, немаркированные данные не являются пустой тратой времени, поскольку в данных могут быть скрыты некоторые полезные шаблоны. Неконтролируемые алгоритмы машинного обучения используют это свойство непомеченных данных и пытаются найти скрытые свойства.

Одним из многих неконтролируемых алгоритмов является кластеризация. Это самая основная форма неконтролируемого обучения. Люди обычно узнают о неконтролируемом обучении через кластеризацию. Кластеризация широко используется в реальном мире для обнаружения аномалий, выявления неисправностей, изучения генетических закономерностей и т. Д.

Алгоритмы кластеризации различаются по тому, как разные алгоритмы воспринимают сходство и несходство в данных. Они также различаются по тому, как каждый из них отделяет кластеры друг от друга. На основе этих подходов кластеризация может быть широко классифицирована на четыре различных типа. Большинство алгоритмов подпадают под одну из этих категорий. В следующем разделе обсуждаются эти категории одна за другой.

Типы кластеризации
Ниже приведены четыре различных типа методов кластеризации, которые обрабатывают все типы данных по-разному. Давайте разберемся в каждом из них.

Кластеризация на основе плотности
Кластеризация на основе плотности опирается на тот факт, что аналогичные точки данных существуют в непосредственной близости, образуя области с высокой плотностью, разделенные областями с низкой плотностью. Алгоритмы кластеризации на основе плотности пытаются найти такие области с высокой плотностью и назвать каждую из таких областей кластерами.

Кластеризация на основе плотности (источник из Википедии)

Рис 2: Кластеризация на основе плотности (взято из Википедии)

Пусть Eps — максимальный радиус кластера. Пусть 'minPts' — минимальное число точек данных в кластере. Тогда

NEPS = {k ε D, dist(i, k) <= EPS}, где D — набор данных всех точек данных.

Говорят, что все точки данных, принадлежащие NEPS, доступны непосредственно по плотности.

Основные особенности кластеризации на основе плотности заключаются в следующем.

Кластеризация на основе плотности может образовывать кластеры произвольной формы. Он не ограничивается какой-либо конкретной формой.
Этот метод не пытается присвоить кластерам выбросы, обеспечивая тем самым эффективное средство анализа выбросов.
Можно определить параметры плотности.
Примерами алгоритмов кластеризации на основе плотности являются DBSCAN и OPTICS, обсуждаемые далее в этой статье.
Кластеризация на основе распределения
Метод кластеризации на основе распределения учитывает распределение набора данных. Каждая точка данных имеет вероятность принадлежности к кластеру. Вероятность уменьшается по мере увеличения расстояния до точки данных от центроида кластера.

Кластеризация на основе распределения — это мягкий метод кластеризации, который не определяет жестких границ между кластерами. Однако можно явно сделать такой алгоритм для выполнения жесткой кластеризации, определив правило, которому необходимо следовать, чтобы принадлежать кластеру. Например, можно задать пороговую вероятность или явно поместить точку в кластер с наибольшей вероятностью.

Однако этот тип алгоритма кластеризации полезен, когда у вас есть представление о распределении ваших данных. В противном случае лучше пойти с каким-то другим типом алгоритма.

Кластеризация на основе распределения (источник из Википедии)

Рис 3: Кластеризация на основе распределения (взято из Википедии)

Кластеризация на основе центроидов
Кластеризация на основе центроидов является одним из наиболее часто используемых методов кластеризации. Вероятно, это первый тип алгоритма кластеризации, с которым вы, возможно, столкнулись на пути обучения кластеризации. При кластеризации на основе центроидов точки данных назначаются кластерам в зависимости от их расстояния от центроида кластера.

Один из самых популярных алгоритмов кластеризации, кластеризация k-средних, является алгоритмом на основе центроидов. Он эффективен и быстр, но чувствителен к выбросам, и результаты могут варьироваться в зависимости от начальных параметров, заданных алгоритму.

Кластеризация на основе центроидов (взято из Википедии)
Рис 4: Кластеризация на основе центроидов (источник из Википедии)

Иерархическая кластеризация
Иерархическая кластеризация — это метод кластеризации на основе дерева, при котором дерево кластеров формируется из заданных данных. Этот тип кластеризации больше подходит для данных, где встречается естественная иерархия, такая как таксономии. Этот тип кластеризации образует хорошо организованные кластеры, но он является более ограничительным по своей природе, чем другие методы.

Наследственная кластеризация (источник из Википедии)
Рис 5: Иерархическая кластеризация (источник из Википедии)

Другие классификации алгоритмов кластеризации
Алгоритмы кластеризации также можно дифференцировать по следующему признаку.

Жесткая кластеризация и мягкая кластеризация
Жесткая кластеризация: При кластеризации данных, если каждая из точек данных принадлежит одному конкретному кластеру, то этот тип кластеризации называется жесткой кластеризацией. Каждая точка данных может принадлежать кластеру или не принадлежать ему. Вероятность существования точки данных в кластере отсутствует. Поэтому существует жесткая граница между различными кластерами.
Мягкая кластеризация: Мягкая кластеризация — это метод нечеткой кластеризации, при котором существование точки данных в кластере определяется с точки зрения вероятности. Точка данных существует в разных кластерах с разной вероятностью, в отличие от жесткой кластеризации. Поэтому между кластерами нет жесткой границы.
Монотетическая кластеризация и полиэтиленовая кластеризация
Монотетическая кластеризация: В этом типе кластеризации члены кластера имеют некоторые общие свойства. Данные делятся между различными кластерами на основе значений общего признака.
Полиэтиленовая кластеризация: В этом типе кластеризации члены кластера проявляют некоторую степень сходства, но могут не иметь общего свойства. Члены кластера назначаются в зависимости от степени сходства между комбинацией набора признаков.
Популярные алгоритмы кластеризации
Существует ряд популярных алгоритмов кластеризации, которые можно использовать для различных вариантов использования. Эти алгоритмы различаются по тому, как они подходят к кластеризации. Каждый алгоритм имеет свои плюсы и минусы. Разберемся в некоторых из самых популярных алгоритмов кластеризации с помощью кода.

Установка библиотеки
В этой статье мы будем использовать популярную библиотеку scikit-learn для реализации различных алгоритмов кластеризации в Python. Мы также сгенерируем случайный набор данных из 1000 образцов, используя метод 'make_classification()' набора данных scikit-learn для подачи в алгоритм кластеризации.

Поэтому библиотеку scikit-learn необходимо установить в системе с помощью следующей команды:

$ pip install scikit-learn
Кроме того, необходимо установить библиотеку numpy и matplotlib, чтобы можно было обрабатывать и просматривать данные. Это можно сделать, выполнив следующую команду:

$ pip install numpy
$ pip install matplotlib
Кластеризация K-средних
Кластеризация K-means является одним из самых популярных алгоритмов машинного обучения без присмотра. Люди часто знакомятся с кластеризацией с помощью кластеризации K-среднего значения. Работу кластеризации k-средних можно понять простыми шагами, приведенными ниже.

Алгоритм кластеризации k-средних принимает в качестве аргумента число кластеров k.
«Средства» в k-средних по существу относятся к усреднению данных путем нахождения центроидов.
Алгоритм начинается со случайных центроидов, которые служат начальными точками для кластеров. Центроиды могут быть реальными точками данных или воображаемыми местоположениями.
Затем он пересчитывает центроиды кластера через каждую итерацию.
Мерой расстояния, используемой в кластеризации k-средних, часто является евклидово расстояние, которое может быть рассчитано ниже.



Алгоритм пересчитывает центроид каждого кластера путем усреднения всех точек данных кластеров.



Алгоритм останавливается, когда,

Кластеризация завершена, поэтому изменений значений центроидов нет.
Количество итераций достигнуто.
Давайте рассмотрим код Python для кластеризации k-средних.

import numpy as np
from sklearn.datasets import make_classification
from sklearn.cluster import KMeans
from matplotlib import pyplot

# 2 features, 2 informative, 0 redundant, 1 cluster per class
X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                           n_redundant=0, n_clusters_per_class=1, random_state=10) 

# 2 clusters
m = KMeans(n_clusters=2) 
# fit the model
m.fit(X)
# predict the cluster for each data point
p = m.predict(X) 
# unique clusters
cl = np.unique(p)
# plot the data points and cluster centers
for c in cl:
    r = np.where(c == p)
    pyplot.title('K-means (No. of Clusters = 3)')
    pyplot.scatter(X[r, 0], X[r, 1])
# show the plot
pyplot.show()
Выпуск:



Рис 6: Результаты кластеризации K-средних, когда количество кластеров, переданных алгоритму, равно 2 и 3 соответственно.

Вы можете ознакомиться с этим туториалом, где мы выполнили сегментацию изображений с помощью кластеризации K-средних.

Мини-пакетная кластеризация K-означает
Мини-пакетная кластеризация k-средних является вариантом кластеризации k-средних. В каждой итерации алгоритм извлекает предопределенное количество случайных выборок из набора данных и сохраняет их в памяти. Эти образцы затем используются для обновления кластеров и их центроидов до тех пор, пока алгоритм не сойдется таким образом, что в кластерах не произойдет существенных изменений.

Преимущества алгоритма мини-пакетов K-средних приведены ниже.

Алгоритм не хранит полный набор данных в памяти.
На каждой итерации алгоритм вычисляет только расстояние между центроидами кластера и образцами в мини-пакете.
Таким образом, алгоритм мини-пакетных k-средних может значительно превзойти традиционный алгоритм кластеризации k-средних.

Реализуем мини-пакетный алгоритм кластеризации k-средних с помощью библиотеки scikit-learn на Python.

import numpy as np
from sklearn.datasets import make_classification
from sklearn.cluster import MiniBatchKMeans
from matplotlib import pyplot

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                           n_redundant=0, n_clusters_per_class=1, random_state=10)
# 3 clusters
m = MiniBatchKMeans(n_clusters=3) 
# fit the model
m.fit(X)
# predict the cluster for each data point
p = m.predict(X) 
# unique clusters
cl = np.unique(p)
# plot the data points and cluster centers
for c in cl:
    r = np.where(c == p)
    pyplot.title('Mini Batch K-means')
    pyplot.scatter(X[r, 0], X[r, 1])
# show the plot
pyplot.show()
Выпуск:



Рис 7: Результаты алгоритма Mini Batch K Means для числа кластеров как 2 и 3 соответственно.

Сравнение времени, затрачиваемого K-средними и мини-пакетными K-средними алгоритмами

Выполним наблюдательный анализ времени, затрачиваемого на кластеризацию K-средних и мини-пакетный алгоритм кластеризации K-средних для кластеризации данных.

import numpy as np
from sklearn.datasets import make_classification
from sklearn.cluster import MiniBatchKMeans
from sklearn.cluster import KMeans
from matplotlib import pyplot
import timeit

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                           n_redundant=0, n_clusters_per_class=1, random_state=10)
# start timer for Mini Batch K-Means
t1_mkm = timeit.default_timer() 
m = MiniBatchKMeans(n_clusters=2)
m.fit(X)
p = m.predict(X)
# stop timer for Mini Batch K-Means
t2_mkm = timeit.default_timer()
# start timer for K-Means
t1_km = timeit.default_timer()
m = KMeans(n_clusters=2)
m.fit(X)
p = m.predict(X)
# stop timer for K-Means
t2_km = timeit.default_timer()
# print time difference
print("Time difference between Mini Batch K-Means and K-Means = ",
      (t2_km-t1_km)-(t2_mkm-t1_mkm))
Выпуск:

Time difference between Mini Batch K-Means and K-Means =  0.024629213998196064
Распространение аффинити
Вы уже видели, что с помощью K-средних нужно заранее указать количество кластеров. Если у вас нет знаний о количестве кластеров, которые могут существовать в данных, вы можете выбрать алгоритм распространения сходства, а не K-средние, поскольку он не нуждается в таких параметрах. По этой же причине этот алгоритм завоевал популярность.

Алгоритм действует следующим образом.

Все точки данных устанавливаются в качестве потенциальных примеров в начале.
Точки данных отправляют сообщения во все другие точки данных и получают сообщения от всех других точек данных.
Затем точка решает, на основе полученных сообщений, точку, с которой она хотела бы связать себя.
Затем это решение отправляется отправителю первоначального сообщения, информируя о его доступности.
Точка, которая отправляет сообщение о его доступности, затем становится примером для отправителя исходного сообщения.
Все точки данных, имеющие один и тот же пример, связаны с одним и тем же кластером.
Алгоритм вычисляет матрицу подобия, матрицу ответственности, матрицу доступности и матрицу критериев, чтобы придумать кластеры. Подробную информацию об алгоритме вы можете найти здесь.

Рассмотрим пример кластеризации с помощью распространения аффинити в Python:

import numpy as np
from sklearn.datasets import make_classification
from sklearn.cluster import AffinityPropagation
from matplotlib import pyplot

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                           n_redundant=0, n_clusters_per_class=1, random_state=10)

# initialize the model
m = AffinityPropagation(damping=0.9)
# fit the model
m.fit(X)
# predict the cluster for each data point
p = m.predict(X)
# unique clusters
cl = np.unique(p)
# plot the data points and cluster centers
for c in cl:
    r = np.where(c == p)
    pyplot.title('Affinity Propagation Clustering')
    pyplot.scatter(X[r, 0], X[r, 1])
# show the plot
pyplot.show()
Выпуск:

Результат алгоритма кластеризации распространения сходства

Рис 8: Результат алгоритма кластеризации распространения аффинности

Алгоритм кластеризации DBSCAN
Алгоритм K-средних является мощным и популярным алгоритмом кластеризации. Но алгоритм K-средних чувствителен к выбросам, поскольку он в конечном итоге помещает все точки данных в некоторый кластер. Из-за этого выбросы могут существенно повлиять на конечные кластеры. Еще одна проблема с алгоритмом k-средних заключается в том, что вы должны предоставить алгоритму количество кластеров. Но много раз у вас не было бы этих знаний.

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) значительно смягчает вышеупомянутые недостатки. Преимущества алгоритма DBSCAN приведены ниже.

Вам не нужно указывать количество кластеров для алгоритма. Он определяет это сам по себе.
Алгоритм DBSCAN может создавать кластеры произвольной формы.
Он гораздо менее чувствителен к выбросам. DBSCAN не назначает кластерам принудительно все точки данных.
Идея алгоритма DBSCAN заключается в том, что подобные точки данных расположены в плотных областях, разделенных областями с более низкой плотностью. Таким образом, он относится к категории алгоритмов кластеризации на основе плотности. Алгоритм нуждается в двух параметрах, как указано ниже.

минПтс: Минимальное количество точек, необходимое для маркировки региона как кластера. Таким образом, вы можете контролировать плотность своих кластеров.
Эпсилон: Он означает расстояние, которое делает две точки близкими друг к другу. С помощью этих параметров можно управлять чувствительностью к выбросам.
В алгоритме DBSCAN есть концепция подключения и доступности. Доступность указывает, может ли точка данных быть достигнута из другой точки данных прямо или косвенно, тогда как связь указывает, находятся ли две точки данных в одном кластере.

На основании этих мер можно классифицировать баллы следующим образом.

Основной момент: Точка считается основной точкой, если существует, по крайней мере, количество точек данных «minPts» в радиусе «эпсилона» от точки.
Пограничный пункт: Точка является пограничной точкой, если в радиусе «эпсилона» меньше точек minPts, но она достижима из какой-либо другой основной точки.
Посторонний: Точка, которая не является ни основной точкой, ни пограничной точкой, называется выбросом.
Вы можете прочитать больше о процессе алгоритма DBSACN здесь.

Давайте посмотрим на реализацию алгоритма DBSCAN в Python:

import numpy as np
from sklearn.datasets import make_classification
from sklearn.cluster import DBSCAN
from matplotlib import pyplot

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                           n_redundant=0, n_clusters_per_class=1, random_state=10)
# init the model
m = DBSCAN(eps=0.05, min_samples=10)
# predict the cluster for each data point after fitting the model
p = m.fit_predict(X) 
# unique clusters
cl = np.unique(p)
# plot the data points and cluster centers
for c in cl:
    r = np.where(c == p)
    pyplot.title('DBSCAN Clustering')
    pyplot.scatter(X[r, 0], X[r, 1])
# show the plot
pyplot.show()
Выпуск:



Рис 9: Результат кластеризации DBSCAN с eps = 0.05 и 0.1 соответственно

ОПТИКА
OPTICS (Ordering Points To Identify Clustering Structure) можно назвать преемником алгоритма DBSCAN. Это еще один алгоритм кластеризации на основе плотности, который разделяет кластеры на основе плотности точек данных.

Алгоритм DBSCAN предполагает, что кластеры имеют фиксированную плотность, что действует как слабость алгоритма. С другой стороны, OPTICS позволяет создавать кластеры различной плотности. Алгоритм OPTICS добавляет еще две концепции к алгоритму DBSCAN, как указано ниже.

Расстояние до ядра: Основная точка, которая классифицируется как основная точка, должна находиться в пределах основного расстояния. Для точки, которая не является основной точкой, эта переменная не определена.
Расстояние досягаемости: Расстояние достижимости между двумя основными точками данных a и b представляет собой максимальное расстояние между a и b и расстояние до ядра a.
Подробнее об алгоритме OPTICS можно прочитать здесь. Рассмотрим пример кода алгоритма OPTICS на Python:

import numpy as np
from sklearn.datasets import make_classification
from sklearn.cluster import OPTICS
from matplotlib import pyplot

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                           n_redundant=0, n_clusters_per_class=1, random_state=10)

# init the model
m = OPTICS(eps=0.5, min_samples=10)
# predict the cluster for each data point after fitting the model
p = m.fit_predict(X)
# unique clusters
cl = np.unique(p)
# plot the data points and cluster centers
for c in cl:
    r = np.where(c == p)
    pyplot.title('OPTICS Clustering')
    pyplot.scatter(X[r, 0], X[r, 1])
# show the plot
pyplot.show()
Выпуск:



Рис 10: Результат кластеризации OPTICS с eps = 0,9 и 0,5 соответственно

БЕРЕЗА
BIRCH (Balance Iterative Reducing and Clustering Hierarchies) — иерархический алгоритм кластеризации. Алгоритм сначала генерирует сводку большого набора данных, сохраняя большую часть информации, насколько это возможно. Затем он формирует кластеры из обобщенных данных. По этой причине BIRCH используется исключительно для больших наборов данных.

Однако основным недостатком BIRCH является то, что он не может обрабатывать категориальные атрибуты. Он предназначен для обработки только атрибутов метрики.

Алгоритм BIRCH представляет собой двухэтапный алгоритм. Он работает, сначала создавая дерево CF (Clustering Feature), а затем выполняя глобальную кластеризацию. BIRCH пытается уменьшить использование памяти, производя дерево CF из областей плотности. Каждая запись дерева CF состоит из трех параметров, как указано ниже.

где

N = количество элементов данных в кластере,

LS = линейная сумма всех элементов данных в кластере, и

SS = квадратная сумма всех элементов данных в кластере.

Как только дерево сформировано, алгоритм переходит к следующему шагу, т.е. глобальной кластеризации.

Алгоритм принимает три параметра для выполнения кластеризации. Эти параметры приведены ниже.

Пороговое значение: это максимальное количество элементов данных, которое листовой узел дерева CF может содержать в подкластере.
Коэффициент ветвления: это максимальное количество подкластеров в каждом узле дерева CF.
Количество кластеров: этот параметр обозначает общее количество кластеров, которые алгоритм создает после конечного результата, т.е. после глобальной кластеризации. Если для этого параметра задано значение None, алгоритм возвращает промежуточные кластеры без выполнения глобальной кластеризации.
Если вас заинтересовала статья от авторов BIRCH, вы можете посетить здесь.

Давайте посмотрим на кластеризацию с использованием алгоритма BIRCH в Python:

import numpy as np
from sklearn.datasets import make_classification
from sklearn.cluster import Birch
from matplotlib import pyplot

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                           n_redundant=0, n_clusters_per_class=1, random_state=10)
# init the model with 2 clusters
m = Birch(threshold=0.05, n_clusters=2)
# predict the cluster for each data point after fitting the model
p = m.fit_predict(X) 
# unique clusters
cl = np.unique(p)
# plot the data points and cluster centers
for c in cl:
    r = np.where(c == p)
    pyplot.title('Birch Clustering')
    pyplot.scatter(X[r, 0], X[r, 1])
# show the plot
pyplot.show()
Выпуск:



Рис 11: Результат алгоритма кластеризации BIRCH с различными пороговыми значениями

Агломеративная кластеризация
Агломеративная кластеризация является, пожалуй, самым базовым и часто используемым алгоритмом иерархической кластеризации. Алгоритм создает дерево из набора данных путем слияния похожих точек данных в один кластер до тех пор, пока все точки данных не будут объединены в один кластер. Алгоритм работает так, как показано ниже.

Алгоритм начинается с рассмотрения всех точек данных как разных кластеров.
Два кластера, похожие по метрике подобия, объединяются в один кластер.
Второй шаг повторяется до тех пор, пока все точки данных не будут объединены в один кластер.
Результирующее дерево, полученное алгоритмом, называется дендрограммой. Как вы, возможно, заметили, агломеративная кластеризация создает дерево снизу вверх. Еще один алгоритм, называемый алгоритмом разделительной кластеризации, создает дерево сверху вниз.

Мера расстояния важна при работе с алгоритмами иерархической кластеризации. Вы можете измерить расстояние между двумя кластерами, используя различные методы связывания, как показано ниже.

Одинарный рычажный механизм: Он обозначает кратчайшее расстояние между ближайшими точками данных двух кластеров.


Рис 12: Одинарный рычажный механизм

Полная связь: Он обозначает самое большое расстояние между точками данных двух кластеров.


Рис 13: Полное соединение

Средняя связь: Как следует из названия, это среднее значение всех парных расстояний точек данных двух кластеров.
Центроидная связь: Это расстояние между центроидами двух скоплений.


Рис 14: Центроидная связь

Давайте посмотрим на код агломеративной кластеризации в Python:

import numpy as np
from sklearn.datasets import make_classification
from sklearn.cluster import AgglomerativeClustering
from matplotlib import pyplot

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                           n_redundant=0, n_clusters_per_class=1, random_state=10)
# init the model with 3 clusters
m = AgglomerativeClustering(n_clusters=3)
# predict the cluster for each data point after fitting the model
p = m.fit_predict(X) 
# unique clusters
cl = np.unique(p)
# plot the data points and cluster centers
for c in cl:
    r = np.where(c == p)
    pyplot.title('Agglomerative Clustering')
    pyplot.scatter(X[r, 0], X[r, 1])
# show the plot
pyplot.show()
Выпуск:



Рис 15: Результаты агломеративной кластеризации с числом кластеров 2 и 3 соответственно

Среднесмещатая кластеризация
Алгоритм Mean-Shift работает путем смещения центроида кластеров в сторону областей более высокой плотности последовательно в каждой итерации. Алгоритм обнаруживает большие двоичные объекты точек данных в областях плотности. Это алгоритм на основе центроидов, который выбирает кандидатов для центроидов и продолжает обновлять их до тех пор, пока не произойдет обновления.

Вы можете проследить работу алгоритма, выполнив заданные шаги.

Первым шагом является создание кластера скользящих окон для каждого элемента данных в наборе данных.
В каждой итерации центроид скользящего окна обновляется и смещается в сторону областей высокой плотности. Эти центроиды предназначены для обозначения точек данных соответствующего скользящего окна.
Второй шаг повторяется до тех пор, пока не будут обнаружены области более высокой плотности.
На этапе постобработки раздвижные окна выбираются путем удаления перекрывающихся раздвижных окон. Если несколько раздвижных окон перекрываются, алгоритм выбирает окно наибольшей плотности и удаляет другие окна.
Наконец, кластеры формируются из раздвижных окон.
Алгоритм среднего сдвига основан на оценке плотности ядра (KDE), где каждая точка данных действует как функция плотности вероятности. Поскольку алгоритм смещает окно в сторону областей с высокой плотностью, его также называют алгоритмом восхождения на холмы.

Алгоритм среднего сдвига дает отличные результаты с данными сферической формы. Одним из основных преимуществ этого алгоритма является то, что вам не нужно указывать количество кластеров в качестве параметра инициализации, как в кластеризации k-средних. Кроме того, выходные данные алгоритма не смещены в сторону параметров инициализации.

Тем не менее, алгоритм среднего сдвига не очень масштабируем, поскольку он требует нескольких поисков для поиска ближайших соседей в каждой операции сдвига окна. Подробнее об алгоритме можно узнать из статьи, опубликованной ее авторами.

Давайте посмотрим на алгоритм среднего сдвига в Python:

import numpy as np
from sklearn.datasets import make_classification
from sklearn.cluster import MeanShift
from matplotlib import pyplot

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                           n_redundant=0, n_clusters_per_class=1, random_state=10)
# init the model
m = MeanShift()
# predict the cluster for each data point after fitting the model
p = m.fit_predict(X)
# unique clusters
cl = np.unique(p)
# plot the data points and cluster centers
for c in cl:
    r = np.where(c == p)
    pyplot.title('Mean Shift Clustering')
    pyplot.scatter(X[r, 0], X[r, 1])
# show the plot
pyplot.show()
Выпуск:



Рис 16: Результат кластеризации среднего сдвига

Спектральная кластеризация
Алгоритм спектральной кластеризации в последнее время создает ажиотаж из-за его простоты, эффективности, способности решаться линейной алгеброй и производительности по сравнению с классическими алгоритмами кластеризации.

Алгоритм спектральной кластеризации очень часто используется при исследовательском анализе данных и уменьшении размерности сложных и многомерных данных. Алгоритм использует графический подход к кластеризации, где он идентифицирует точки данных, которые связаны или находятся непосредственно рядом друг с другом. Как только такие узлы идентифицированы, они используются для формирования кластеров в более низких измерениях.

Одним из преимуществ спектральной кластеризации является то, что она не делает никаких предположений о форме скопления. Поэтому он более точен. Однако алгоритм менее масштабируем для больших наборов данных, поскольку сложность увеличивается, а точность снижается. Этапы спектральной кластеризации приведены ниже.

Алгоритм сначала создает матрицу расстояний от точек данных.
Затем он создает матрицу аффинности из матрицы расстояния и вычисляет матрицу градусов и матрицу Лапласа.
Затем алгоритм генерирует собственные значения и собственные векторы матрицы Лапласа.
Наконец, матрица формируется из собственных векторов с наибольшими собственными значениями 'k'. Затем алгоритм нормализует векторы.
На последнем шаге он формирует кластеры из точек данных в k-мерном пространстве.
 Рассмотрим пример кода спектральной кластеризации в Python:

import numpy as np
from sklearn.datasets import make_classification
from sklearn.cluster import SpectralClustering
from matplotlib import pyplot

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                           n_redundant=0, n_clusters_per_class=1, random_state=10)
# init the model with 3 clusters
m = SpectralClustering(n_clusters=3)
# predict the cluster for each data point after fitting the model
p = m.fit_predict(X)
# unique clusters
cl = np.unique(p)
# plot the data points and cluster centers
for c in cl:
    r = np.where(c == p)
    pyplot.title('Spectral Clustering')
    pyplot.scatter(X[r, 0], X[r, 1])
# show the plot
pyplot.show()
Выпуск:



Рис 17: Результат спектральной кластеризации с числом кластеров как 2 и 3 соответственно

Модель смеси Гаусса
Модель гауссовской смеси основана на гауссовом распределении данных. Модель пытается подогнать различные модели Гаусса к данным и узнать такие параметры, как среднее и дисперсия распределений. Затем алгоритм использует эти параметры для расчета вероятностей существования точек данных в разных кластерах.

Модель смеси Гаусса похожа, но отличается от кластеризации k-средних. Если быть точным, он имеет два основных преимущества перед алгоритмом кластеризации k-средних.

Первое преимущество заключается в том, что модель смеси Гаусса учитывает дисперсию данных, а k-средние — нет.
Во-вторых, алгоритм k-средних представляет собой жесткий алгоритм кластеризации, в котором точка данных либо существует в кластере, либо не принадлежит кластеру. Однако модель смеси Гаусса дает вероятности существования точки данных в кластере. Поэтому это мягкий метод кластеризации.
Модель смеси Гаусса работает следующим образом.

Алгоритм сначала инициализирует параметры среднего, ковариационного и смешанного коэффициентов случайным образом.
Затем он вычисляет значения присвоения коэффициентов для всех гауссов.
Оцените параметры еще раз, используя вычисленное присвоение коэффициента.
Затем алгоритм вычисляет функцию логарифмической вероятности и определяет некоторые правила о критерии сходимости.
Если значение вероятности журнала сходится, оно останавливается и создает кластеры, в противном случае алгоритм снова начинается с шага 2.
Давайте посмотрим на Python-код кластеризации модели смеси Гаусса:

import numpy as np
from sklearn.datasets import make_classification
from sklearn.mixture import GaussianMixture
from matplotlib import pyplot

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,
                           n_redundant=0, n_clusters_per_class=1, random_state=10)
# init the model with 2 components
m = GaussianMixture(n_components=2)
# predict the cluster for each data point after fitting the model
p = m.fit_predict(X)
# unique clusters
cl = np.unique(p)
# plot the data points and cluster centers
for c in cl:
    r = np.where(c == p)
    pyplot.title('Gaussian Mixture Clustering')
    pyplot.scatter(X[r, 0], X[r, 1])
# show the plot
pyplot.show()
Выпуск:



Рис 18: Результат кластеризации модели смеси Гаусса

Метрики для оценки алгоритмов кластеризации
После получения кластеров из алгоритма кластеризации необходимо оценить результаты. Существуют определенные метрики для оценки результатов кластеризации. Давайте совершим экскурсию по каждому из них.

Оценка однородности
Показатель однородности оценивает кластеры на основе меток классов членов кластера. Если все точки данных во всех кластерах принадлежат к одному классу, говорят, что он удовлетворяет условию однородности.

Показатель однородности лежит между 0 и 1, где 0 полностью неоднороден, а 1 совершенно однородн. Вы можете рассчитать показатель однородности, используя уравнение, приведенное ниже.



Оценка полноты
Если все точки данных класса лежат в одних и тех же кластерах, то результаты кластеризации, как говорят, удовлетворяют свойству полноты. Оценка полноты может быть рассчитана по формуле, приведенной ниже.



Оценка полноты также находится между 0 и 1, где 1 обозначает идеальную полноту, а 0 означает отсутствие полноты вообще.

V Оценка измерения
Оценка v-measure устанавливает правильность алгоритма кластеризации. Это гармоническое среднее между оценкой полноты и оценкой однородности. Формула для оценки v-measure приведена ниже.



Оценка v-меры находится между 0 и 1, где 1 означает, что кластеризация идеальна. Вы можете получить эти метрики алгоритмов кластеризации, используя встроенные функции библиотеки scikit-learn, как указано в примере ниже.

from sklearn import metrics

y_true = [5, 3, 5, 4, 4, 5]
y_pred = [3, 5, 5, 4, 3, 4]
# homogeneity: each cluster contains only members of a single class.
print(metrics.homogeneity_score(y_true, y_pred))
# completeness: all members of a given class are assigned to the same cluster.
print(metrics.completeness_score(y_true, y_pred))
# v-measure: harmonic mean of homogeneity and completeness
print(metrics.v_measure_score(y_true, y_pred))
Выпуск:

0.3146685210384134
0.28969008214284747
0.30166311599620416
Примеры использования кластеризации
Данные, создаваемые реальными приложениями и процессами, в значительной степени не помечены. Поэтому кластеризация имеет большое количество практических вариантов использования. В общем, если у вас есть немаркированные данные и есть вероятность найти скрытое секционирование в данных, вы можете рассмотреть кластеризацию в качестве первого подхода. Некоторые из практических вариантов использования кластеризации перечислены ниже.

Сегментация клиентов: корпорации используют кластеризацию для получения результатов сегментации клиентов, чтобы можно было эффективно сформулировать маркетинговые стратегии.
Биология: Генетика изучается с использованием кластеризации в биологии.
Медицина: Кластеризация помогает в поиске значимых результатов в лекарственном взаимодействии и производстве лекарств.
Сжатие изображения: кластеризация помогает сохранить информацию при одновременном снижении размерности изображения.
Библиотека: Книги по различным темам и предметам в библиотеке управляются с помощью кластеризации.
Анализ документов: Исследовательские работы и другие документы могут быть сгруппированы вместе на основе сходства между ними с использованием методов кластеризации.
Лучший алгоритм кластеризации и вещи, которые следует иметь в виду
После просмотра различных алгоритмов кластеризации вам может быть интересно, какой алгоритм является лучшим алгоритмом кластеризации. Ответ на ваш вопрос заключается в том, что не существует единого идеального алгоритма кластеризации, который соответствует всем потребностям.

Различные алгоритмы кластеризации имеют разные преимущества и недостатки. Некоторые из них просты, а другие сложны, некоторые нуждаются в параметрах инициализации, а другие нет, некоторые хорошо масштабируются, а другие не так хорошо масштабируются, и так далее.

Поэтому выбор алгоритма кластеризации зависит от вашего варианта использования. Однако при выборе алгоритма учитывайте следующие моменты:

Если вашему приложению нужны быстрые результаты, необходимо выбрать алгоритм меньшей временной сложности.
Если распределение данных неизвестно, следует избегать алгоритмов кластеризации на основе распределения.
Если у вас очень большой набор данных, вы должны выбрать алгоритм, который хорошо масштабируется.
Заключение
В статье обсуждались категории алгоритмов кластеризации, большинство популярных алгоритмов кластеризации, а также метрики для оценки этих алгоритмов. Вы также видели практические варианты использования кластеризации и моменты, которые следует помнить при выборе алгоритма. К настоящему времени вы, надеюсь, получили достаточно знаний, чтобы эффективно использовать кластеризацию в своем приложении.