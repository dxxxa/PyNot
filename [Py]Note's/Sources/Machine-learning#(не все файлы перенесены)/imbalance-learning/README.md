# [Imbalance Learning With Imblearn and Smote Variants Libraries in Python](https://www.thepythoncode.com/article/handling-imbalance-data-imblearn-smote-variants-python)
To run this:
- `pip3 install -r requirements.txt`
- Download and extract `creditcard.csv` from this [Kaggle link](https://www.kaggle.com/mlg-ulb/creditcardfraud)
- Then, you're free to run `imb_learning.py` as you wish.
##
# [[] / []]()
В машинном обучении, а точнее в классификации (контролируемое обучение), промышленные / необработанные наборы данных, как известно, имеют дело с гораздо большим количеством сложностей по сравнению с игрушечными данными.

Среди этих ограничений — наличие высокого коэффициента дисбаланса, где обычно общие классы встречаются гораздо чаще (большинство), чем те, которые мы фактически нацелены на изучение (меньшинство).

В этом уроке мы углубимся в более подробную информацию о том, что лежит под проблемой обучения Imbalance, как она влияет на наши модели, поймем, что мы подразумеваем под under/oversampling и реализуем с использованием smote-вариантов библиотеки Python.

На протяжении всего учебника мы будем использовать мошеннический набор данных кредитных карт от Kaggle, который вы можете скачать здесь.

Установка

$ pip install numpy pandas imblearn smote-variants
Узнайте также: Выбор функций с помощью Scikit-Learn в Python

Что такое обучение дисбалансу?

Дисбаланс обучения в большинстве случаев присутствует в отрасли. Как упоминалось выше, целевые классы (либо для двоичных, либо для многоклассовых задач), которые нуждаются в дальнейшем изучении и анализе, обычно страдают от недостатка данных перед огромным присутствием общих классов.

Этот дисбаланс оказывает прямое негативное влияние на производительность модели во время обучения, где она будет смещена в сторону класса большинства, и это может привести к падению под парадокс точности.

Чтобы лучше проиллюстрировать парадокс точности, представьте себе набор данных, который содержит 98 выборок из класса 0 и 2 из класса 1, если наша модель наивно предсказывает все их как 0, мы все равно будем иметь точность 98%, что относительно хорошо, хотя наша модель просто предсказывала по умолчанию все классы как 0. Такое значение точности вводит в заблуждение и может дать неправильные выводы.

По этой причине мы теперь рассмотрим используемые метрики, когда речь заходит о несбалансированных наборах данных:

Точность: Точность помогает, когда затраты на ложные срабатывания высоки, поэтому чем больше точность, тем меньше частота ложных срабатываний.
Напомним: Отзыв помогает, когда стоимость ложноотрицательных результатов высока, поэтому чем больше отзыв, тем меньше уровень ложноотрицательных результатов.
F1-Score: это гармоническое среднее точности и запоминания, другими словами, оно учитывает оба из них, поэтому чем выше балл F1, тем ниже показатели ложноотрицательных и ложноположительных результатов.


Другая характеристика, присутствующая в обучении дисбалансу, известна как коэффициент дисбаланса. В случае бинарной несбалансированной классификации она рассчитывается путем деления размера класса меньшинства на размер класса большинства. Мы используем IR, чтобы узнать, насколько серьезна проблема дисбаланса.

Типы понижения/перевыборки

Чтобы уменьшить коэффициент дисбаланса, мы можем использовать 2 различных подхода, мы можем либо уменьшить классы большинства (недостаточная выборка), либо добавить выборки к меньшинствам (перевыборка).

В этом уроке мы рассмотрим 2 типа подходов на основе ресамплинга:

Случайная выборка: Здесь мы не следуем заданной эвристике, мы стохастически удаляем или дублируем образцы.
Направленная выборка: В отличие от случайной выборки, здесь мы следуем заданному эвристическому алгоритму, чтобы выбрать выборки, которые необходимо удалить в случае недостаточной выборки, или создать и сгенерировать искусственно точки данных для меньшинств при выполнении избыточной выборки.
Для случайной выборки мы будем использовать библиотеку imblearn Python. Но сначала мы превратим наш CSV в формат фрейма данных и сохраним метки в переменную y.

import numpy as np
import pandas as pd

df=pd.read_csv("creditcard.csv")

y=df["Class"]
X=df.drop(["Time","Class"],axis=1)
print(y.value_counts())
Выпуск:

0    284315
1       492
Name: Class, dtype: int64
Теперь мы создадим объект RandomUnderSampler() из imblearn и используем метод fit_resample() для применения недовыборки к набору данных.

from imblearn.under_sampling import RandomUnderSampler

under=RandomUnderSampler()

X_und,y_und=under.fit_resample(X,y)

print(len(X_und[X_und==1])==len(X_und[X_und==0]))
Выпуск:

True
Как вы можете заметить, после применения недовыборки количество немошеннических кредитных карт приравнивается к мошенническим.

Для случайной передискретизации мы используем один и тот же процесс, единственное отличие заключается в использовании RandomOverSampler() вместо RandomUnderSampler().

from imblearn.over_sampling import RandomOverSampler

over=RandomOverSampler()

X_und,y_und=over.fit_resample(X,y)

print(len(X_und[X_und==1])==len(X_und[X_und==0]))
Выпуск:

True
Направленный недовыборочный отбор

Теперь мы углубимся во второй тип алгоритмов балансировки, которые являются направленными подходами.

Для недостаточной выборки мы рассмотрим следующие алгоритмы: Отредактированные ближайшие соседи, Порог твердости экземпляра и TomekLinks.

Отредактировано Ближайшие соседи: в этой технике недостаточной выборки мы удаляем выборки большинства, где большая часть его соседей принадлежит к классу меньшинства.


Порог твердости экземпляра: Здесь мы исключаем точки данных из класса большинства, которые трудно классифицировать. Мы в основном используем n классификаторов и усредняем вероятность неправильной классификации экземпляра, которая равна 1 - Pi, где Pi - оценочное предсказание вероятности для выборки i с классификатором.
TomekLinks: Здесь мы удаляем образцы большинства, которые имеют tomeklink с точкой данных меньшинства. Связь томека возникает, когда эта формула соблюдается; учитывая две выборки x и y, для любого другого образца z мы имеем: dist(x,y) < dist(x,z) и dist(x,y) < dist(y,z). Другими словами, точки данных меньшинства и большинства образуют связь томека, если они являются ближайшими соседями друг к другу.
Томек ссылки

from imblearn.under_sampling import EditedNearestNeighbours,InstanceHardnessThreshold,TomekLinks 

under_samp_models=[EditedNearestNeighbours(),InstanceHardnessThreshold,TomekLinks()]
for under_samp_model in under_samp_models:
    X_und,y_und=under_samp_model.fit_resample(X,y)
Направленная передискретизация с использованием Smote

SMOTE ("Synthetic Minority Oversampling TEchnique") - это метод перевыборки, который работает путем проведения линий между точками данных меньшинства и генерации данных по этим линиям, как показано на рисунке ниже.



Мы будем использовать библиотеку Python smote-variants, которая представляет собой пакет, включающий 85 вариантов smote, все упомянутые в этой научной статье.

Реализация очень похожа на реализацию imblearn с незначительными изменениями, такими как использование метода sample() вместо fit_resample() для генерации данных. В этом уроке мы будем использовать Kmeans_Smote, Safe_Level_Smote и Smote_Cosine для примеров.

import smote_variants as sv 

svs=[sv.kmeans_SMOTE(),sv.Safe_Level_SMOTE(),sv.SMOTE_Cosine()]

for over_sampler in svs: 
    X_over_samp, y_over_samp= over_sampler.sample(X, y)
Заключение

На протяжении всего этого урока мы узнали:

Как определить проблему несбалансированности обучения, и что такое коэффициент дисбаланса.
Узнайте, как использовать соответствующие метрики, чтобы не попасть в парадокс точности.
Узнайте разницу между случайной и направленной выборкой.
Используйте библиотеки Python imblearn и smote-варианты для недостаточной и сверхсамплинговой соответственно.