# [Dimensionality Reduction Using Feature Selection in Python](https://www.thepythoncode.com/article/dimensionality-reduction-feature-selection)
##
# [[] / []]()
Знакомство
Выбор компонентов — это процесс выбора подмножества объектов из набора данных, который вносит наибольший вклад в производительность модели, и это без применения к ней какого-либо типа преобразования.

Выбор компонентов разбит на три категории: фильтр, оболочка и встраивание. Методы фильтрации исследуют статистические свойства объектов, чтобы определить, какие из них являются лучшими. Подходы-оболочки используют метод проб и ошибок для выбора подмножества функций, которые обеспечивают наиболее точные модели.

В конце концов, методы встраивания выбирают оптимальное подмножество признаков во время или в качестве расширения фазы обучения алгоритма обучения. В идеале этот учебник должен охватывать все три подхода.

Более глубокий взгляд на конкретные алгоритмы обучения затрудняет описание встроенных подходов до более глубокого изучения этих алгоритмов. Следовательно, мы будем обсуждать только методы выбора функций фильтра и оболочки.

На приведенной ниже диаграмме показаны различные типы выбора объектов.

особенностивыборыИсточник: средний

 

Содержание:

Пороговое значение Отклонение численного признака
Пороговое значение отклонения двоичных признаков
Обработка высококоррелированных функций
Удаление нерелевантных признаков для классификации
Рекурсивное устранение функций
Заключение
Пороговое значение Отклонение численного признака
Предположим, что вы хотите исключить объекты с низкой дисперсией из коллекции числовых признаков. Дисперсионное пороговое значение (VT) является фундаментальным методом выбора признаков. Основываясь на гипотезе о том, что признаки с низкой дисперсией не так интересны (или ценны), как черты с высокой дисперсией, был разработан этот подход. Во-первых, дисперсия каждого признака может быть рассчитана по следующей формуле:

пороговые значения

где x — вектор признака, xi — индивидуальное значение признака, а x̄ — среднее значение этого признака. Он удаляет все признаки, дисперсия которых не соответствует этому критерию.

При использовании VT важно помнить две вещи: эта дисперсия не центрирована, то есть она находится в квадратной единице объекта. Это означает, что наборы функций с несколькими единицами (например, одна функция в годах, а другая в долларах) не будут работать с VT.

Поскольку порог дисперсии должен быть установлен вручную, мы должны полагаться на наш собственный опыт и знания в этом выборе (или использовать технику выбора модели). Вы можете следовать приведенному ниже коду:

# Load libraries
# import data
iris = datasets.load_iris()
# Create features and target
features_i = iris.data
target_i = iris.target
# thresholder  creation
thresholder = VarianceThreshold(threshold=.4)
# high variance feature matrix creation
f_high_variance = thresholder.fit_transform(features_i)
# View high variance feature matrix
f_high_variance[0:3]
array([[5.1, 1.4, 0.2],
       [4.9, 1.4, 0.2],
       [4.7, 1.3, 0.2]])
Дисперсию каждого компонента можно просмотреть с помощью variances_.

# View variances
thresholder.fit(features_i).variances_
Выпуск:

array([0.68112222, 0.18871289, 3.09550267, 0.57713289])
В качестве последнего пункта пороговое значение дисперсии потерпит неудачу, если признаки стандартизированы (т.е. средний ноль и единица дисперсии).

# feature matrix stantardization
scaler = StandardScaler()
f_std = scaler.fit_transform(features_i)
# variance of each feature calculation
selection = VarianceThreshold()
selection.fit(f_std).variances_
array([1., 1., 1., 1.])
Пороговое значение отклонения двоичных признаков
Дисперсионный анализ может быть использован для выбора наиболее полезных категорий, как и в случае с числовыми признаками. Для случайных величин Бернулли (двоичных признаков) для вычисления дисперсии используется следующая формула:

Variance(x) = p(1 − p)

p - доля наблюдений класса 1. Следовательно, регулируя p, мы можем удалить признаки, где большинство наблюдений относятся к первому классу.

Предположим, что была собрана коллекция двоичных категориальных признаков, и мы хотим устранить признаки с низкой дисперсией (т. е. вероятно, содержащие мало информации). Мы выберем атрибуты, которые имеют дисперсию случайной величины Бернулли выше заданного порога:

# feature matrix creation with:
# for Feature 0: 80% class 0
# for Feature 1: 80% class 1
# for Feature 2: 60% class 0, 40% class 1
features_i = [[0, 2, 0],
[0, 1, 1],
[0, 1, 0],
[0, 1, 1],
[1, 0, 0]]
# threshold by variance
thresholding = VarianceThreshold(threshold=(.65 * (1 - .65)))
thresholding.fit_transform(features_i)
array([[2, 0],
       [1, 1],
       [1, 0],
       [1, 1],
       [0, 0]])
Обработка высококоррелированных функций
Если у вас есть матрица признаков и вы чувствуете, что некоторые из признаков сильно коррелируют, просто проверьте наличие высококоррелированных признаков с помощью корреляционной матрицы. Если есть сильно коррелированные черты, попробуйте отказаться от одной из них.

# Create feature matrix with two highly correlated features
features_m = np.array([[1, 1, 1],
[2, 2, 0],
[3, 3, 1],
[4, 4, 0],
[5, 5, 1],
[6, 6, 0],
[7, 7, 1],
[8, 7, 0],
[9, 7, 1]])
# Conversion of  feature matrix
dataframe = pd.DataFrame(features_m)
# correlation matrix creation
corr_m = dataframe.corr().abs()
# upper triangle selection
upper1 = corr_m.where(np.triu(np.ones(corr_m.shape),
k=1).astype(np.bool))
# For correlation greater than 0.85, Find index of feature columns
droping = [col for col in upper1.columns if any(upper1[col] > 0.85)]
# Drop features
dataframe.drop(dataframe.columns[droping], axis=1).head(3)

0   0	2
0	1	1
1	2	0
2	3	1
Сильно коррелированные функции являются распространенной проблемой в машинном обучении. Когда две черты сильно коррелируют, информация, которую они предоставляют, очень похожа, и, вероятно, излишне включать оба.

Удаление одного из коррелированных признаков из набора функций решает проблему. В рамках нашего подхода мы сначала создаем корреляционную матрицу, которая включает в себя все признаки. Мы используем метод dataframe.corr() и смотрим на верхний треугольник корреляционной матрицы, чтобы найти пары сильно коррелированных признаков и исключить один признак из каждой из этих пар.

Удаление нерелевантных признаков для классификации
Предположим, вы хотите удалить неинформативный объект из целевого вектора категории. Можно вычислить статистику хи-квадрата между каждым объектом и целевым вектором, если объекты категоричны.

# Load data
iris_i = load_iris()
features_v = iris.data
target = iris.target
# categorical data coversion
features_v = features_v.astype(int)
# Selection of two features using highest chi-squared 
chi2_s = SelectKBest(chi2, k=2)
f_kbest = chi2_s.fit_transform(features_v, target)
# Show results
print("Original number of features:", features_v.shape[1])
print("Reduced number of features:", f_kbest.shape[1])
Original number of features: 4
Reduced number of features: 2
Определите F-значение ANOVA между каждым объектом и целевым вектором, если они являются количественными.

# Selection of two features using highest F-values
f_selector = SelectKBest(f_classif, k=2)
f_kbest = f_selector.fit_transform(features_v, target)
# Pisplay results
print("Original number of features:", features_v.shape[1])
print("Reduced number of features:", f_kbest.shape[1])
Original number of features: 4
Reduced number of features: 2
SelectPercentile можно использовать для выбора верхних n процентов объектов вместо указанного количества объектов.

# Selection of top 65% of features 
f_selector = SelectPercentile(f_classif, percentile=65)
f_kbest = f_selector.fit_transform(features_v, target)
# Display results
print("Original number of features:", features_v.shape[1])
print("Reduced number of features:", f_kbest.shape[1])
Original number of features: 4
Reduced number of features: 2
Независимость двух векторов категорий исследуется с помощью статистики хи-квадрата:

Площадь ЧиOя — количество наблюдений в классе I. Eя — количество наблюдений в классе I, если предсказать, не связаны ли признак и целевой вектор. Статистика в хи-квадрате — это одно значение, которое информирует вас о том, насколько велика разница между вашими наблюдаемыми подсчетами и подсчетами, которые вы бы предсказали, если бы в популяции не было никакой связи.

Вычисление хи-квадратной статистики между объектом и целевым вектором дает оценку независимости между ними. Если цель не связана с переменной признака, она не имеет для нас смысла, поскольку она не содержит информации, которая может быть использована для классификации. Если эти две функции сильно зависят, они, вероятно, будут особенно информативными для обучения нашей модели.

Статистика хи-квадрата используется для выбора объектов путем определения того, насколько хорошо каждый признак коррелирует с целевым вектором, а затем выбирает признаки с использованием лучшей статистики хи-квадрата.

Параметр k определяет, сколько функций мы хотим сохранить. Чтобы использовать хи-квадрат для выбора объектов, целевой вектор и объекты должны быть категориальными. Мы можем использовать подход хи-квадрат, если у нас есть числовое значение, которое может быть преобразовано в категориальный признак, а метод хи-квадрат требует, чтобы все значения были неотрицательными.
SelectKBest может использоваться для выбора наиболее статистически значимых функций.
Кроме того, f_classif могут быть использованы для вычисления статистики F-значения ANOVA для каждого объекта и целевого вектора, когда доступны числовые признаки. Оценки F-значения анализируют, существенно ли отличаются средства числовых признаков, когда мы группируем их по целевому вектору.
Рекурсивное устранение функций
Вы хотите автоматически выбирать лучшие функции. RFECV может использоваться для выполнения рекурсивного устранения признаков (RFE) с перекрестной проверкой (CV).

RFE построен на идее многократного обучения модели, такой как линейная регрессия или машины опорных векторов, которые включают параметры (также известные как веса или коэффициенты). Мы включаем все функции в первое обучение модели. Чтобы исключить объект из набора функций, мы ищем объект с наименьшим параметром (отмечая, что это предполагает, что объекты должны быть либо перемасштабированы, либо стандартизированы).

Добавление новой концепции, называемой перекрестной проверкой (CV), необходимо для лучшей стратегии. Очевидно, сколько функций мы должны сохранить? Повторение этого цикла до тех пор, пока не останется только один признак, возможно (в теории).

Перекрестная проверка используется для определения оптимального количества функций, которые необходимо сохранить во время RFE. В RFE с CV мы используем перекрестную проверку для тестирования нашей модели после каждой итерации. Если CV демонстрирует, что наша модель улучшилась после того, как мы удалили функцию, мы переходим к следующему циклу. Однако, если CV указывает, что наша модель ухудшилась после удаления функции, мы восстанавливаем эту функцию в наборе функций и выбираем эти функции как лучшие.

RFE с CV реализован с использованием класса RFECV() и включает в себя ряд ключевых параметров:

Параметр оценщика указывает тип модели, подлежащей обучению (например, линейная регрессия).
Алгоритм пошаговой регрессии определяет количество или долю объектов, которые должны быть отброшены в каждом цикле.
Параметр оценки указывает показатель качества, который будет использоваться для оценки нашей модели во время перекрестной проверки.
# Load libraries
# Suppress an annoying but harmless warning
warnings.filterwarnings(action="ignore", module="scipy",
message="^internal gelsd")
#  features matrix, target vector, true coefficients
features_f, target_t = make_regression(n_samples = 10000,
n_features = 100,
n_informative = 2,
random_state = 1)
# linear regression creation
ols = linear_model.LinearRegression()
# Recursive features elimination
rfecv = RFECV(estimator=ols, step=2, scoring="neg_mean_squared_error")
rfecv.fit(features_f, target_t)
rfecv.transform(features_f)
array([[ 0.00850799,  0.7031277 ],
       [-1.07500204,  2.56148527],
       [ 1.37940721, -1.77039484],
       ...,
       [-0.80331656, -1.60648007],
       [ 0.39508844, -1.34564911],
       [-0.55383035,  0.82880112]])
Следуя процессу RFE, мы можем иметь лучшее представление о том, сколько функций мы должны сохранить:

# Number of best features
rfecv.n_features_
2
Мы можем дополнительно определить, какие из этих черт должны быть сохранены:

# What the best categories ?
rfecv.support_
array([False, False, False, False, False,  True, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False,  True, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False, False, False, False, False, False, False, False, False,
       False])
Мы даже можем просмотреть рейтинги функций:

# We can even see how the features are ranked
rfecv.ranking_
array([16, 18, 43, 40, 13,  1, 26, 22, 44, 31, 23, 33, 22, 32, 14, 12, 35,
        2, 29, 11, 11, 41,  3, 37, 46, 34, 45, 48,  4, 12,  2, 49, 16, 10,
       40, 20, 14, 15,  5,  1, 21,  6, 45, 47, 29, 42, 17,  6,  7, 10, 24,
       41, 47, 19,  8,  8, 26, 43, 49, 46, 28, 19,  4, 24, 48,  3, 33, 42,
        5, 38, 27, 31, 50, 30,  9, 50, 17, 23,  7, 30, 34,  9, 28, 37, 20,
       13, 21, 25, 38, 39, 32, 39, 36, 36, 15, 27, 44, 35, 18, 25])
Заключение
Выбор функций является критическим этапом в разработке моделей машинного обучения. Это может сократить время обучения, упростить наши модели, упростить их отладку и сократить время выхода на рынок решений машинного обучения.

Полный код можно найти здесь.