# [Credit Card Fraud Detection in Python](https://www.thepythoncode.com/article/credit-card-fraud-detection-using-sklearn-in-python#near-miss)
To run this:
- `pip3 install -r requirements.txt`
- Download [the dataset from Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud)
- Read through the tutorial and run the cells on `CreditCardDetection.ipynb`.
##
# [[] / []]()
Знакомство
Фирмы, выпускающие кредитные карты, должны выявлять мошеннические транзакции по кредитным картам, чтобы предотвратить взимание с потребителей платы за продукты, которые они не покупали. Наука о данных может решить такую проблему, и ее значение в сочетании с машинным обучением нельзя подчеркнуть.

Этот учебник полностью написан на версии Python 3. Для каждого фрагмента кода дается соответствующее описание. Тем не менее, читатель, как ожидается, будет иметь предыдущий опыт работы с Python. Мы всегда старались предоставить краткую теоретическую основу относительно методологий, используемых в этом учебнике. Давайте начнем.

Вот оглавление:

Знакомство
Набор данных обнаружения мошенничества с кредитными картами
Исследование и визуализация данных
Подготовка данных
Построение и обучение модели
Недовыборка
Методы NearMiss
Передискретизация с помощью SMOTE
Приложение: Обнаружение и удаление выбросов
Заключение
Набор данных обнаружения мошенничества с кредитными картами
Мы будем использовать набор данных обнаружения мошенничества с кредитными картами от Kaggle. Используемый набор данных охватывает транзакции по кредитным картам, совершенные европейскими держателями карт в сентябре 2013 года. Этот набор данных содержит 492 мошенничества из 284 807 транзакций за два дня. Набор данных несбалансирован, причем положительный класс (мошенничество) составляет 0,172 процента всех транзакций. Для загрузки набора данных необходимо создать учетную запись Kaggle. Я также загрузил набор данных на Google Диск, к которому вы можете получить доступ здесь.

После загрузки набора данных поместите его в текущий рабочий каталог. Давайте установим требования:

$ pip install sklearn==0.24.2 imbalanced-learn numpy pandas matplotlib seaborn
Импортируем необходимые библиотеки:

# Importing modules 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import gridspec
Теперь мы читаем данные и пытаемся понять значение каждой функции. Панды модуля Python предоставляют нам функции для чтения данных. На следующем шаге мы прочитаем данные из нашего каталога, где хранятся данные, а затем посмотрим на первую и последние пять строк данных с помощью методов head() и tail():

#read the dataset
dataset = pd.read_csv("creditcard.csv")
# read the first 5 and last 5 rows of the data
dataset.head().append(dataset.tail())
╔════════╤══════════╤════════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤════════╤═══════╗
║        │ Time     │ V1         │ V2        │ V3        │ V4        │ V5        │ V6        │ V7        │ V8        │ V9        │ ... │ V21       │ V22       │ V23       │ V24       │ V25       │ V26       │ V27       │ V28       │ Amount │ Class ║
╠════════╪══════════╪════════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪════════╪═══════╣
║ 0      │ 0.0      │ -1.359807  │ -0.072781 │ 2.536347  │ 1.378155  │ -0.338321 │ 0.462388  │ 0.239599  │ 0.098698  │ 0.363787  │ ... │ -0.018307 │ 0.277838  │ -0.110474 │ 0.066928  │ 0.128539  │ -0.189115 │ 0.133558  │ -0.021053 │ 149.62 │ 0     ║
╟────────┼──────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼────────┼───────╢
║ 1      │ 0.0      │ 1.191857   │ 0.266151  │ 0.166480  │ 0.448154  │ 0.060018  │ -0.082361 │ -0.078803 │ 0.085102  │ -0.255425 │ ... │ -0.225775 │ -0.638672 │ 0.101288  │ -0.339846 │ 0.167170  │ 0.125895  │ -0.008983 │ 0.014724  │ 2.69   │ 0     ║
╟────────┼──────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼────────┼───────╢
║ 2      │ 1.0      │ -1.358354  │ -1.340163 │ 1.773209  │ 0.379780  │ -0.503198 │ 1.800499  │ 0.791461  │ 0.247676  │ -1.514654 │ ... │ 0.247998  │ 0.771679  │ 0.909412  │ -0.689281 │ -0.327642 │ -0.139097 │ -0.055353 │ -0.059752 │ 378.66 │ 0     ║
╟────────┼──────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼────────┼───────╢
║ 3      │ 1.0      │ -0.966272  │ -0.185226 │ 1.792993  │ -0.863291 │ -0.010309 │ 1.247203  │ 0.237609  │ 0.377436  │ -1.387024 │ ... │ -0.108300 │ 0.005274  │ -0.190321 │ -1.175575 │ 0.647376  │ -0.221929 │ 0.062723  │ 0.061458  │ 123.50 │ 0     ║
╟────────┼──────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼────────┼───────╢
║ 4      │ 2.0      │ -1.158233  │ 0.877737  │ 1.548718  │ 0.403034  │ -0.407193 │ 0.095921  │ 0.592941  │ -0.270533 │ 0.817739  │ ... │ -0.009431 │ 0.798278  │ -0.137458 │ 0.141267  │ -0.206010 │ 0.502292  │ 0.219422  │ 0.215153  │ 69.99  │ 0     ║
╟────────┼──────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼────────┼───────╢
║ 284802 │ 172786.0 │ -11.881118 │ 10.071785 │ -9.834783 │ -2.066656 │ -5.364473 │ -2.606837 │ -4.918215 │ 7.305334  │ 1.914428  │ ... │ 0.213454  │ 0.111864  │ 1.014480  │ -0.509348 │ 1.436807  │ 0.250034  │ 0.943651  │ 0.823731  │ 0.77   │ 0     ║
╟────────┼──────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼────────┼───────╢
║ 284803 │ 172787.0 │ -0.732789  │ -0.055080 │ 2.035030  │ -0.738589 │ 0.868229  │ 1.058415  │ 0.024330  │ 0.294869  │ 0.584800  │ ... │ 0.214205  │ 0.924384  │ 0.012463  │ -1.016226 │ -0.606624 │ -0.395255 │ 0.068472  │ -0.053527 │ 24.79  │ 0     ║
╟────────┼──────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼────────┼───────╢
║ 284804 │ 172788.0 │ 1.919565   │ -0.301254 │ -3.249640 │ -0.557828 │ 2.630515  │ 3.031260  │ -0.296827 │ 0.708417  │ 0.432454  │ ... │ 0.232045  │ 0.578229  │ -0.037501 │ 0.640134  │ 0.265745  │ -0.087371 │ 0.004455  │ -0.026561 │ 67.88  │ 0     ║
╟────────┼──────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼────────┼───────╢
║ 284805 │ 172788.0 │ -0.240440  │ 0.530483  │ 0.702510  │ 0.689799  │ -0.377961 │ 0.623708  │ -0.686180 │ 0.679145  │ 0.392087  │ ... │ 0.265245  │ 0.800049  │ -0.163298 │ 0.123205  │ -0.569159 │ 0.546668  │ 0.108821  │ 0.104533  │ 10.00  │ 0     ║
╟────────┼──────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼────────┼───────╢
║ 284806 │ 172792.0 │ -0.533413  │ -0.189733 │ 0.703337  │ -0.506271 │ -0.012546 │ -0.649617 │ 1.577006  │ -0.414650 │ 0.486180  │ ... │ 0.261057  │ 0.643078  │ 0.376777  │ 0.008797  │ -0.473649 │ -0.818267 │ -0.002415 │ 0.013649  │ 217.00 │ 0     ║
╚════════╧══════════╧════════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧════════╧═══════╝
Время измеряется в секундах с момента первой транзакции в сборе данных. В результате мы можем сделать вывод, что этот набор данных содержит все транзакции, записанные в течение двух дней. Признаки были подготовлены с использованием PCA, поэтому физическая интерпретация отдельных признаков не имеет смысла. «Время» и «Количество» являются единственными функциями, которые не трансформируются в PCA. 'Class' — это переменная ответа, и она имеет значение 1, если имеет место мошенничество, и 0 в противном случае.

Исследование и визуализация данных
Теперь попробуем выяснить относительную долю действительных и мошеннических транзакций по кредитным картам:

# check for relative proportion 
print("Fraudulent Cases: " + str(len(dataset[dataset["Class"] == 1])))
print("Valid Transactions: " + str(len(dataset[dataset["Class"] == 0])))
print("Proportion of Fraudulent Cases: " + str(len(dataset[dataset["Class"] == 1])/ dataset.shape[0]))

# To see how small are the number of Fraud transactions
data_p = dataset.copy()
data_p[" "] = np.where(data_p["Class"] == 1 ,  "Fraud", "Genuine")

# plot a pie chart
data_p[" "].value_counts().plot(kind="pie")
Fraudulent Cases: 492
Valid Transactions: 284315
Proportion of Fraudulent Cases: 0.001727485630620034
Классы данных

Существует дисбаланс в данных, и только 0,17% от общего числа случаев являются мошенническими.

Теперь рассмотрим распределение двух названных объектов в наборе данных. Для Time понятно, что в тот день, когда происходило большинство транзакций, была определенная продолжительность:

# plot the named features 
f, axes = plt.subplots(1, 2, figsize=(18,4), sharex = True)

amount_value = dataset['Amount'].values # values
time_value = dataset['Time'].values # values

sns.distplot(amount_value, hist=False, color="m", kde_kws={"shade": True}, ax=axes[0]).set_title('Distribution of Amount')
sns.distplot(time_value, hist=False, color="m", kde_kws={"shade": True}, ax=axes[1]).set_title('Distribution of Time')

plt.show()
Распределение времени и количества

Давайте проверим, есть ли разница между действительными транзакциями и мошенническими транзакциями:

print("Average Amount in a Fraudulent Transaction: " + str(dataset[dataset["Class"] == 1]["Amount"].mean()))
print("Average Amount in a Valid Transaction: " + str(dataset[dataset["Class"] == 0]["Amount"].mean()))
Average Amount in a Fraudulent Transaction: 122.21132113821133
Average Amount in a Valid Transaction: 88.29102242225574
Как мы можем заметить из этого, средняя денежная транзакция для мошеннических больше. Это делает эту проблему крайне важной для решения. Теперь попробуем разобраться в распределении значений в каждом признаке. Начнем с Суммы:

print("Summary of the feature - Amount" + "\n-------------------------------")
print(dataset["Amount"].describe())
Summary of the feature - Amount
-------------------------------
count    284807.000000
mean         88.349619
std         250.120109
min           0.000000
25%           5.600000
50%          22.000000
75%          77.165000
max       25691.160000
Name: Amount, dtype: float64
Остальные особенности не имеют никакой физической интерпретации и будут видны через гистограммы. Здесь значения подгруппированы по классу (допустимый или мошеннический):

# Reorder the columns Amount, Time then the rest
data_plot = dataset.copy()
amount = data_plot['Amount']
data_plot.drop(labels=['Amount'], axis=1, inplace = True)
data_plot.insert(0, 'Amount', amount)

# Plot the distributions of the features
columns = data_plot.iloc[:,0:30].columns
plt.figure(figsize=(12,30*4))
grids = gridspec.GridSpec(30, 1)
for grid, index in enumerate(data_plot[columns]):
 ax = plt.subplot(grids[grid])
 sns.distplot(data_plot[index][data_plot.Class == 1], hist=False, kde_kws={"shade": True}, bins=50)
 sns.distplot(data_plot[index][data_plot.Class == 0], hist=False, kde_kws={"shade": True}, bins=50)
 ax.set_xlabel("")
 ax.set_title("Distribution of Column: "  + str(index))
plt.show()
Распространение функций PCA

Подготовка данных
Поскольку функции создаются с помощью PCA, выбор функций не нужен, так как многие функции крошечные. Давайте посмотрим, есть ли в наборе данных какие-либо недостающие значения:

# check for null values
dataset.isnull().shape[0]
print("Non-missing values: " + str(dataset.isnull().shape[0]))
print("Missing values: " + str(dataset.shape[0] - dataset.isnull().shape[0]))
Non-missing values: 284807
Missing values: 0
Поскольку недостающих данных нет, мы обращаемся к стандартизации. Мы стандартизируем только время и количество с помощью RobustScaler:

from sklearn.preprocessing import RobustScaler
scaler = RobustScaler().fit(dataset[["Time", "Amount"]])
dataset[["Time", "Amount"]] = scaler.transform(dataset[["Time", "Amount"]])

dataset.head().append(dataset.tail())
Как мы видели ранее, столбец Amount имеет выбросы, поэтому мы выбрали RobustScaler(), поскольку он устойчив к выбросам. Выпуск:

╔════════╤═══════════╤════════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════════╤═══════╗
║        │ Time      │ V1         │ V2        │ V3        │ V4        │ V5        │ V6        │ V7        │ V8        │ V9        │ ... │ V21       │ V22       │ V23       │ V24       │ V25       │ V26       │ V27       │ V28       │ Amount    │ Class ║
╠════════╪═══════════╪════════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════════╪═══════╣
║ 0      │ -0.994983 │ -1.359807  │ -0.072781 │ 2.536347  │ 1.378155  │ -0.338321 │ 0.462388  │ 0.239599  │ 0.098698  │ 0.363787  │ ... │ -0.018307 │ 0.277838  │ -0.110474 │ 0.066928  │ 0.128539  │ -0.189115 │ 0.133558  │ -0.021053 │ 1.783274  │ 0     ║
╟────────┼───────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────╢
║ 1      │ -0.994983 │ 1.191857   │ 0.266151  │ 0.166480  │ 0.448154  │ 0.060018  │ -0.082361 │ -0.078803 │ 0.085102  │ -0.255425 │ ... │ -0.225775 │ -0.638672 │ 0.101288  │ -0.339846 │ 0.167170  │ 0.125895  │ -0.008983 │ 0.014724  │ -0.269825 │ 0     ║
╟────────┼───────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────╢
║ 2      │ -0.994972 │ -1.358354  │ -1.340163 │ 1.773209  │ 0.379780  │ -0.503198 │ 1.800499  │ 0.791461  │ 0.247676  │ -1.514654 │ ... │ 0.247998  │ 0.771679  │ 0.909412  │ -0.689281 │ -0.327642 │ -0.139097 │ -0.055353 │ -0.059752 │ 4.983721  │ 0     ║
╟────────┼───────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────╢
║ 3      │ -0.994972 │ -0.966272  │ -0.185226 │ 1.792993  │ -0.863291 │ -0.010309 │ 1.247203  │ 0.237609  │ 0.377436  │ -1.387024 │ ... │ -0.108300 │ 0.005274  │ -0.190321 │ -1.175575 │ 0.647376  │ -0.221929 │ 0.062723  │ 0.061458  │ 1.418291  │ 0     ║
╟────────┼───────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────╢
║ 4      │ -0.994960 │ -1.158233  │ 0.877737  │ 1.548718  │ 0.403034  │ -0.407193 │ 0.095921  │ 0.592941  │ -0.270533 │ 0.817739  │ ... │ -0.009431 │ 0.798278  │ -0.137458 │ 0.141267  │ -0.206010 │ 0.502292  │ 0.219422  │ 0.215153  │ 0.670579  │ 0     ║
╟────────┼───────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────╢
║ 284802 │ 1.034951  │ -11.881118 │ 10.071785 │ -9.834783 │ -2.066656 │ -5.364473 │ -2.606837 │ -4.918215 │ 7.305334  │ 1.914428  │ ... │ 0.213454  │ 0.111864  │ 1.014480  │ -0.509348 │ 1.436807  │ 0.250034  │ 0.943651  │ 0.823731  │ -0.296653 │ 0     ║
╟────────┼───────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────╢
║ 284803 │ 1.034963  │ -0.732789  │ -0.055080 │ 2.035030  │ -0.738589 │ 0.868229  │ 1.058415  │ 0.024330  │ 0.294869  │ 0.584800  │ ... │ 0.214205  │ 0.924384  │ 0.012463  │ -1.016226 │ -0.606624 │ -0.395255 │ 0.068472  │ -0.053527 │ 0.038986  │ 0     ║
╟────────┼───────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────╢
║ 284804 │ 1.034975  │ 1.919565   │ -0.301254 │ -3.249640 │ -0.557828 │ 2.630515  │ 3.031260  │ -0.296827 │ 0.708417  │ 0.432454  │ ... │ 0.232045  │ 0.578229  │ -0.037501 │ 0.640134  │ 0.265745  │ -0.087371 │ 0.004455  │ -0.026561 │ 0.641096  │ 0     ║
╟────────┼───────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────╢
║ 284805 │ 1.034975  │ -0.240440  │ 0.530483  │ 0.702510  │ 0.689799  │ -0.377961 │ 0.623708  │ -0.686180 │ 0.679145  │ 0.392087  │ ... │ 0.265245  │ 0.800049  │ -0.163298 │ 0.123205  │ -0.569159 │ 0.546668  │ 0.108821  │ 0.104533  │ -0.167680 │ 0     ║
╟────────┼───────────┼────────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼─────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────────┼───────╢
║ 284806 │ 1.035022  │ -0.533413  │ -0.189733 │ 0.703337  │ -0.506271 │ -0.012546 │ -0.649617 │ 1.577006  │ -0.414650 │ 0.486180  │ ... │ 0.261057  │ 0.643078  │ 0.376777  │ 0.008797  │ -0.473649 │ -0.818267 │ -0.002415 │ 0.013649  │ 2.724796  │ 0     ║
╚════════╧═══════════╧════════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════════╧═══════╝
10 rows × 31 columns
Далее давайте разделим данные на объекты и цели. Мы также делаем разделение данных на поезд-тест:

# Separate response and features  Undersampling before cross validation will lead to overfiting
y = dataset["Class"] # target 
X = dataset.iloc[:,0:30]

# Use SKLEARN for the split
from sklearn.model_selection import train_test_split 
X_train, X_test, y_train, y_test = train_test_split( 
        X, y, test_size = 0.2, random_state = 42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape
Выпуск:

((227845, 30), (56962, 30), (227845,), (56962,))
Давайте импортируем все необходимые библиотеки для учебника:

# Create the cross validation framework 
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV

kf = StratifiedKFold(n_splits=5, random_state = None, shuffle = False)
# Import the imbalance Learn module
from imblearn.pipeline import make_pipeline ## Create a Pipeline using the provided estimators .
from imblearn.under_sampling import NearMiss  ## perform Under-sampling  based on NearMiss methods. 
from imblearn.over_sampling import SMOTE  ## PerformOver-sampling class that uses SMOTE. 
# import the metrics
from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, recall_score, precision_score, f1_score
# Import the classifiers
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
Построение и обучение модели
Запустим RandomForestClassifier на наборе данных и посмотрим на производительность:

# Fit and predict
rfc = RandomForestClassifier() 
rfc.fit(X_train, y_train) 
y_pred = rfc.predict(X_test)

# For the performance let's use some metrics from SKLEARN module
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
  
print("The accuracy is", accuracy_score(y_test, y_pred)) 
print("The precision is", precision_score(y_test, y_pred))
print("The recall is", recall_score(y_test, y_pred))
print("The F1 score is", f1_score(y_test, y_pred))
Тренировка должна занять несколько минут, чтобы закончить. Вот выходные данные:

The accuracy is 0.9996137776061234
The precision is 0.975
The recall is 0.7959183673469388
The F1 score is 0.8764044943820225
Как вы можете видеть, у нас было только 0,17% мошеннических транзакций, а модель, предсказывающая, что все транзакции будут действительными, будет иметь точность 99,83%. К счастью, наша модель превысила этот показатель до более чем 99,96%.

В результате точность не является подходящей метрикой для нашей проблемы. Есть еще три:

Точность: Это общее число истинных положительных результатов, деленное на истинные положительные и ложноположительные. Точность гарантирует, что мы не выявим хорошие транзакции как мошеннические в нашей проблеме.
Вспоминать: Это общее число истинных позитивов, деленное на истинные положительные и ложноотрицательные. Напомним, что мы не предсказываем мошеннические транзакции как все хорошее и, следовательно, получаем хорошую точность с ужасной моделью.
Результат Формулы-1: Это гармоническое средство точности и запоминания. Это хорошее среднее значение между обеими метриками.
Отзыв важнее точности в нашей проблеме, так как прогнозирование мошеннической транзакции как хорошей хуже, чем пометка хорошей транзакции как мошеннической, вы можете использовать fbeta_score() и настроить бета-параметр, чтобы сделать его более взвешенным для отзыва.

В следующих разделах мы сделаем сетку и рандомизированный поиск по сверхдискретизации и недодискретизации на различных классификаторах.

Недовыборка
В этом разделе мы выполним недовыборку для нашего набора данных. Один тривиальный момент, который следует отметить, заключается в том, что мы не будем недооценивать данные тестирования, поскольку мы хотим, чтобы наша модель хорошо работала с искаженными распределениями классов.

Эти шаги заключаются в следующем:

Используйте 5-кратную перекрестную проверку на тренировочном наборе.
На каждой из складок используйте недодискретизацию.
Поместите модель на обучающие складки и проверьте на проверочной складке.
Методы NearMiss
Imbalanced-Learn — это модуль Python, который помогает балансировать наборы данных, которые сильно искажены или смещены в сторону определенных классов. Это помогает в ресамплинге классов, которые обычно имеют избыточную или недодискретизированную выборку. Если коэффициент дисбаланса выше, выходные данные наклоняются в сторону класса с наибольшим количеством выборок. Просмотрите этот учебник, чтобы узнать больше о модуле несбалансированного обучения.

Near Miss относится к группе стратегий недостаточной выборки, которые выбирают выборки на основе расстояния между экземплярами класса большинства и меньшинства.

В приведенном ниже коде мы создаем гибкую функцию, которая может выполнять сетку или рандомизированный поиск по заданному оценщику и его параметрам с или без недостаточной / избыточной выборки и возвращает лучший оценщик вместе с метриками производительности:

def get_model_best_estimator_and_metrics(estimator, params, kf=kf, X_train=X_train, 
                                         y_train=y_train, X_test=X_test, 
                                         y_test=y_test, is_grid_search=True, 
                                         sampling=NearMiss(), scoring="f1", 
                                         n_jobs=2):
    if sampling is None:
        # make the pipeline of only the estimator, just so the remaining code will work fine
        pipeline = make_pipeline(estimator)
    else:
        # make the pipeline of over/undersampling and estimator
        pipeline = make_pipeline(sampling, estimator)
    # get the estimator name
    estimator_name = estimator.__class__.__name__.lower()
    # construct the parameters for grid/random search cv
    new_params = {f'{estimator_name}__{key}': params[key] for key in params}
    if is_grid_search:
        # grid search instead of randomized search
        search = GridSearchCV(pipeline, param_grid=new_params, cv=kf, return_train_score=True, n_jobs=n_jobs, verbose=2)
    else:
        # randomized search
        search = RandomizedSearchCV(pipeline, param_distributions=new_params, 
                                    cv=kf, scoring=scoring, return_train_score=True,
                                    n_jobs=n_jobs, verbose=1)
    # fit the model
    search.fit(X_train, y_train)
    cv_score = cross_val_score(search, X_train, y_train, scoring=scoring, cv=kf)
    # make predictions on the test data
    y_pred = search.best_estimator_.named_steps[estimator_name].predict(X_test)
    # calculate the metrics: recall, accuracy, F1 score, etc.
    recall = recall_score(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    y_proba = search.best_estimator_.named_steps[estimator_name].predict_proba(X_test)[::, 1]
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    auc = roc_auc_score(y_test, y_proba)
    # return the best estimator along with the metrics
    return {
        "best_estimator": search.best_estimator_,
        "estimator_name": estimator_name,
        "cv_score": cv_score,
        "recall": recall,
        "accuracy": accuracy,
        "f1_score": f1,
        "fpr": fpr,
        "tpr": tpr,
        "auc": auc,
    }
Поскольку никогда не бывает достаточно данных для обучения модели, исключение ее части для проверки приводит к несоответствию. Мы рискуем потерять важные шаблоны / тенденции в наборе данных, снижая обучающие данные, что увеличивает ошибку, вызванную предвзятостью. Таким образом, нам нужна стратегия, которая предлагает достаточно данных для обучения модели, одновременно оставляя достаточно данных для проверки.

Функция cross_val_score() использует перекрестную проверку для определения оценки, которую мы используем в приведенной выше функции. Ознакомьтесь с этим руководством, чтобы узнать больше об этой функции.

Функция сделана гибкой. Например, если вы хотите выполнить поиск по сетке в модели LogisticRegression с недостаточной выборкой, вы просто используете следующее:

# Cumulatively create a table for the ROC curve
## Create the dataframe
res_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])

logreg_us_results = get_model_best_estimator_and_metrics(
    estimator=LogisticRegression(),
    params={"penalty": ['l1', 'l2'], 
                  'C': [ 0.01, 0.1, 1, 100], 
                  'solver' : ['liblinear']},
    sampling=NearMiss(),
)
print(f"==={logreg_us_results['estimator_name']}===")
print("Model:", logreg_us_results['best_estimator'])
print("Accuracy:", logreg_us_results['accuracy'])
print("Recall:", logreg_us_results['recall'])
print("F1 Score:", logreg_us_results['f1_score'])
res_table = res_table.append({'classifiers': logreg_us_results["estimator_name"],
                                        'fpr': logreg_us_results["fpr"], 
                                        'tpr': logreg_us_results["tpr"], 
                                        'auc': logreg_us_results["auc"]
                              }, ignore_index=True)
Если вы хотите отключить недодискретизацию, вы просто передаете None параметру sampling в функции get_model_best_estimator_and_metrics().

Если вы хотите построить кривую ROC на нескольких моделях, вы можете запустить приведенный выше код для нескольких моделей и их параметров. Просто не забудьте отредактировать имя классификатора, чтобы различать модели с избыточной и недостаточной выборкой.

Мне удалось запустить пять различных моделей по недодискретизации (на тренировку ушло много часов), и вот как мы строим нашу кривую ROC, используя наши res_table:

# Plot the ROC curve for undersampling
res_table.set_index('classifiers', inplace=True)
fig = plt.figure(figsize=(17,7))

for j in res_table.index:
    plt.plot(res_table.loc[j]['fpr'], 
             res_table.loc[j]['tpr'], 
             label="{}, AUC={:.3f}".format(j, res_table.loc[j]['auc']))
    
plt.plot([0,1], [0,1], color='orange', linestyle='--')
plt.xticks(np.arange(0.0, 1.1, step=0.1))
plt.xlabel("Positive Rate(False)", fontsize=15)
plt.yticks(np.arange(0.0, 1.1, step=0.1))
plt.ylabel("Positive Rate(True)", fontsize=15)
plt.title('Analysis for Oversampling', fontweight='bold', fontsize=15)
plt.legend(prop={'size':13}, loc='lower right')
plt.show()
Кривая ROC для различных моделей по недодискретизации по проблеме кредитной карты

Они были обучены с использованием undersampling NearMiss() и на пяти различных моделях. Таким образом, если вы запустите приведенный выше код, вы увидите только одну кривую для LogisticRegression; Убедитесь, что вы скопировали эту ячейку и сделали это для других моделей, если хотите.

Передискретизация с помощью SMOTE
Одна из проблем с несбалансированной классификацией заключается в том, что существует слишком мало выборок класса меньшинства, чтобы модель могла успешно изучить границу принятия решения. Передискретизация экземпляров из класса меньшинства является одним из решений проблемы. Перед установкой модели мы дублируем образцы из класса меньшинства в обучающем наборе.

Синтез новых примеров из класса меньшинства является улучшением по сравнению с повторением примеров из класса меньшинства. Это особенно эффективный тип увеличения данных для табличных данных. В этой статье показано, что сочетание чрезмерной выборки класса меньшинства и недостаточной выборки класса большинства может улучшить производительность классификатора.

Аналогично вы можете передать SMOTE() параметру выборки в нашей функции для перевыборки:

# Cumulatively create a table for the ROC curve
res_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])

lin_reg_os_results = get_model_best_estimator_and_metrics(
    estimator=LogisticRegression(),
    params={"penalty": ['l1', 'l2'], 'C': [ 0.01, 0.1, 1, 100, 100], 
            'solver' : ['liblinear']},
    sampling=SMOTE(random_state=42),
    scoring="f1",
    is_grid_search=False,
    n_jobs=2,
)
print(f"==={lin_reg_os_results['estimator_name']}===")
print("Model:", lin_reg_os_results['best_estimator'])
print("Accuracy:", lin_reg_os_results['accuracy'])
print("Recall:", lin_reg_os_results['recall'])
print("F1 Score:", lin_reg_os_results['f1_score'])
res_table = res_table.append({'classifiers': lin_reg_os_results["estimator_name"],
                                        'fpr': lin_reg_os_results["fpr"], 
                                        'tpr': lin_reg_os_results["tpr"], 
                                        'auc': lin_reg_os_results["auc"]
                              }, ignore_index=True)
Обратите внимание, что мы устанавливаем is_grid_search значение False, так как мы знаем, что это займет очень много времени, поэтому мы используем RandomizedSearchCV. Рассмотрите возможность увеличения n_jobs до более чем двух, если у вас большее количество ядер (в настоящее время экземпляр Google Colab имеет только 2 ядра ЦП).

Приложение: Обнаружение и удаление выбросов
Присутствие выброса иногда влияет на модель и может привести нас к неправильным выводам. Поэтому мы должны смотреть на распределение данных, внимательно следя за выбросами. В этом разделе этого учебника используется метод межквартильного диапазона (IQR) для выявления и удаления выбросов:

# boxplot for two example variables in the dataset

f, axes = plt.subplots(1, 2, figsize=(18,4), sharex = True)

variable1 = dataset["V1"]
variable2 = dataset["V2"]

sns.boxplot(variable1, color="m", ax=axes[0]).set_title('Boxplot for V1')
sns.boxplot(variable2, color="m", ax=axes[1]).set_title('Boxplot for V2')

plt.show()
Выбросы

Получение ассортимента:

# Find the IQR for all the feature variables
# Please note that we are keeping Class variable also in this evaluation, though we know using this method no observation
# be removed based on this variable.

quartile1 = dataset.quantile(0.25)
quartile3 = dataset.quantile(0.75)

IQR = quartile3 - quartile1
print(IQR)
Time      1.000000
V1        2.236015
V2        1.402274
V3        1.917560
V4        1.591981
V5        1.303524
V6        1.166861
V7        1.124512
V8        0.535976
V9        1.240237
V10       0.989349
V11       1.502088
V12       1.023810
V13       1.311044
V14       0.918724
V15       1.231705
V16       0.991333
V17       0.883423
V18       0.999657
V19       0.915248
V20       0.344762
V21       0.414772
V22       1.070904
V23       0.309488
V24       0.794113
V25       0.667861
V26       0.567936
V27       0.161885
V28       0.131240
Amount    1.000000
Class     0.000000
dtype: float64
Теперь, когда у нас есть межквартильный диапазон для каждой переменной, мы удаляем наблюдения со значениями выбросов. Мы использовали «константу выброса» как 3:

# Remove the outliers 
constant = 3
datavalid = dataset[~((dataset < (quartile1 - constant * IQR)) |(dataset > (quartile3 + constant * IQR))).any(axis=1)]
deletedrows = dataset.shape[0] - datavalid.shape[0]
print("We have removed " + str(deletedrows) + " rows from the data as outliers")
Выпуск:

We have removed 53376 rows from the data as outliers
Заключение
В этом учебнике обучались различные классификаторы и выполнялись методы недодискретизации и сверхвыборки после разделения данных на обучающие и тестовые наборы, чтобы решить, какой классификатор более эффективен при обнаружении мошеннических транзакций.

GridSearchCV занимает много времени и поэтому эффективен только при недостаточной выборке, поскольку недосэмплинг не занимает много времени во время обучения. Если вы обнаружите, что это занимает вечность в определенной модели, подумайте о том, чтобы уменьшить параметры, которые вы передали, и используйте randomizedSearchCV для этого (т. Е. Установите is_grid_search значение False в нашей основной функции).

Если вы видите, что поиск оптимальных параметров требует времени (а это так), рассмотрите возможность непосредственного использования модели RandomForestClassifier, так как она относительно быстрее тренируется и обычно не перегружается или не перегружается, как вы видели ранее в учебнике.

Алгоритм SMOTE создает новые синтетические точки из класса меньшинства для достижения количественного баланса с классом большинства. Хотя SMOTE может быть более точным, чем случайная недодискретизация, он не удаляет строки, но будет тратить больше времени на обучение.

Это поможет, если перед перекрестной проверкой не будет проведена перевыборка или недостаточная выборка данных, поскольку вы напрямую влияете на набор проверки путем перевыборки или недостаточной выборки перед использованием перекрестной проверки, что приводит к проблеме утечки данных.

Вы можете получить полный код здесь.